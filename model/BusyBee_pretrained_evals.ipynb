{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AomZhAueQJh5"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda as cuda # This import is for if you have an Nvidia GPU and run on your pc\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "# import timm # This import provides a bunch of prebuilt image models you can use for experimentation/comparisons\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x_ZAA6wokDoa"
   },
   "outputs": [],
   "source": [
    "# Training Settings\n",
    "# How many images we will process concurrently, try to use powers of 2, you may want to lower if you're getting the \"out of memory\" error\n",
    "batch_size = 32\n",
    "# The number of times we iterate over the entire dataset\n",
    "epochs = 5\n",
    "# Learning Rate: We may later want to utilize learning rate decay\n",
    "lr = 3e-5\n",
    "gamma = 0.7\n",
    "# For reproducible results\n",
    "seed = 8743749123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "gkORSZeuROdc",
    "outputId": "3cd9d2c3-bbea-4b9d-ef2c-6caa3d53c88d"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1277130155.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [16]\u001b[1;36m\u001b[0m\n\u001b[1;33m    transforms.ToTensor(),\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# First let's get our dataset\n",
    "\n",
    "# Define the transformations\n",
    "# Need to add more and play around with different transformations\n",
    "# RandomCrop is probably a good data augmentation technique we'll want to use\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224))\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Need to troubleshoot INaturalist dataset -- Loads the whole dataset over 100GB and overflows the colab disk\n",
    "# Taxonomy structure: Domain: Eukaryota, Kingdom: Animalia, Phylum: Arthropoda, Class: Insecta, Order: Hymenoptera, Suborder: Apocrita, Superfamily: Apoidea, Epifamily: Anthophila\n",
    "#dataset = datasets.INaturalist(root='./data', version='2021_train_mini', target_type=\"genus\", transform=transforms.ToTensor(), download=True)\n",
    "#dset_size = len(dataset)\n",
    "\n",
    "\n",
    "\n",
    "# Load Data [Locally Stored]\n",
    "cwd = os.getcwd()\n",
    "path = \"%s/../data/train/\" %(cwd)\n",
    "\n",
    "dataset = datasets.ImageFolder(root=path, transform=transform) # Automatically assigns labels based on sub-directory name\n",
    "\n",
    "# # Generate 2 splits: Train (80%), Test (20%)\n",
    "# # (No hyperparameter validation this time around)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "train, test = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# # Create Data Loaders for splits\n",
    "train_dl = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 265, 500] at entry 0 and [3, 500, 500] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 265, 500] at entry 0 and [3, 500, 500] at entry 1"
     ]
    }
   ],
   "source": [
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZtcKoV__gVF9"
   },
   "outputs": [],
   "source": [
    "# Our classes will be the tribe names\n",
    "classes = [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VU3evGWUM4q"
   },
   "outputs": [],
   "source": [
    "def train_model(model, name):\n",
    "    # Loss Function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # Scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(f\"Training Run [Model: {name}]\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "    # Training Time\n",
    "    start_event = cuda.Event(enable_timing=True)\n",
    "    end_event = cuda.Event(enable_timing=True)\n",
    "    # Begin Clock\n",
    "    start_event.record()\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for data, label in tqdm(bg_train_dl if background else no_bg_train_dl):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output, label)\n",
    "            #training_loss[f\"{name}\"].append(loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == label).float().mean()\n",
    "            #training_accuracy[f\"{name}\"].append(acc)\n",
    "            epoch_accuracy += acc / len(bg_train_dl if background else no_bg_train_dl)\n",
    "            epoch_loss += loss / len(bg_train_dl if background else no_bg_train_dl)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} - loss: {epoch_loss:.4f} - acc: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    # End Clock\n",
    "    end_event.record()\n",
    "    cuda.synchronize() # Wait for GPU operations to complete\n",
    "    time = start_event.elapsed_time(end_event) / 1000 # Convert to seconds\n",
    "    num_examples = batch_size * len(bg_train_dl if background else no_bg_train_dl)\n",
    "    time_per_example = time / (num_examples * epochs)\n",
    "    print(f\"It took {time} seconds to train {name} on {num_examples} examples over {epochs} epochs.\")\n",
    "    print(f\"That averages to {time_per_example} seconds per example\")\n",
    "\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(f\"Test Run [Model: {name}] \")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++\")\n",
    "    accuracies = []\n",
    "    batch_acc = 0\n",
    "    for data, label in tqdm(bg_test_dl if background else no_bg_test_dl):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        output = model(data)\n",
    "        acc = (output.argmax(dim=1) == label).float().mean().cpu().detach().numpy()\n",
    "        batch_acc += acc / len(bg_test_dl if background else no_bg_test_dl)\n",
    "        accuracies.append(batch_acc)\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracies[-1]} - Number of test cases: {len(bg_test_dl if background else no_bg_test_dl) * batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "QlBLbmN6xOhw",
    "outputId": "8272b113-cb8b-4103-a4c0-cc14f2f524b3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a167d8661e59>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Label (numeric):'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dset' is not defined"
     ]
    }
   ],
   "source": [
    "# Model 1: VanillaViT\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "VanillaViT = ViT(\n",
    "    image_size = 128,\n",
    "    patch_size = 8,\n",
    "    num_classes = 6,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)\n",
    "train_model(VanillaViT, \"VanillaViT\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(f\"Attention Visualization\")\n",
    "print(f\"[Model: name]\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(\"\")\n",
    "bg_imgs, no_bg_imgs = load_jpgs()\n",
    "from vit_pytorch.recorder import Recorder\n",
    "v = Recorder(VanillaViT)\n",
    "\"\"\"\n",
    "# Experimenting with attention weight visualization\n",
    "\n",
    "for index, img in enumerate(bg_imgs):\n",
    "    img.unsqueeze(0)\n",
    "    preds, attns = v(img)\n",
    "    attns\n",
    "    attns.shape()\n",
    "    plot_mats(img, attns, 'test', categories[index], 'test_plt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwggSCNrYoMy"
   },
   "outputs": [],
   "source": [
    "# Model 2: SimpleViT\n",
    "from vit_pytorch import SimpleViT\n",
    "\n",
    "SimpleViT = SimpleViT(\n",
    "    image_size = 128,\n",
    "    patch_size = 8,\n",
    "    num_classes = 6,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048\n",
    ").to(device)\n",
    "train_model(SimpleViT, \"SimpleViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TN3hzLfzdAm_"
   },
   "outputs": [],
   "source": [
    "# Model 4: T2TViT\n",
    "from vit_pytorch.t2t import T2TViT\n",
    "\n",
    "T2TViT = T2TViT(\n",
    "    dim = 512,\n",
    "    image_size = 128,\n",
    "    depth = 5,\n",
    "    heads = 8,\n",
    "    mlp_dim = 512,\n",
    "    num_classes = 6,\n",
    "    t2t_layers = ((7, 4), (3, 2), (3, 2)) # tuples of the kernel size and stride of  each consecutive layers of the initial token to token module\n",
    ").to(device)\n",
    "train_model(T2TViT, \"T2TViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "rNUW5-P-d45J",
    "outputId": "783e22f3-a681-4f5b-ddf4-d6f90477585e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b636d4ab5c1c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Actually Training the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Load the image and tell pytorch to keep track of the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "# Model 6: CrossViT\n",
    "from vit_pytorch.cross_vit import CrossViT\n",
    "\n",
    "CrossViT = CrossViT(\n",
    "    image_size = 128,\n",
    "    num_classes = 6,\n",
    "    depth = 4,               # number of multi-scale encoding blocks\n",
    "    sm_dim = 192,            # high res dimension\n",
    "    sm_patch_size = 16,      # high res patch size (should be smaller than lg_patch_size)\n",
    "    sm_enc_depth = 2,        # high res depth\n",
    "    sm_enc_heads = 8,        # high res heads\n",
    "    sm_enc_mlp_dim = 2048,   # high res feedforward dimension\n",
    "    lg_dim = 384,            # low res dimension\n",
    "    lg_patch_size = 64,      # low res patch size\n",
    "    lg_enc_depth = 3,        # low res depth\n",
    "    lg_enc_heads = 8,        # low res heads\n",
    "    lg_enc_mlp_dim = 2048,   # low res feedforward dimensions\n",
    "    cross_attn_depth = 2,    # cross attention rounds\n",
    "    cross_attn_heads = 8,    # cross attention heads\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)\n",
    "train_model(CrossViT, \"CrossViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWvXKdoVsx7E"
   },
   "outputs": [],
   "source": [
    "# Model 7: PiT\n",
    "from vit_pytorch.pit import PiT\n",
    "\n",
    "PiT = PiT(\n",
    "    image_size = 128,\n",
    "    patch_size = 16,\n",
    "    dim = 256,\n",
    "    num_classes = 6,\n",
    "    depth = (3, 3, 3),     # list of depths, indicating the number of rounds of each stage before a downsample\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)\n",
    "train_model(PiT, \"PiT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uITq_SmcppfQ"
   },
   "outputs": [],
   "source": [
    "# Model 8: LeViT\n",
    "from vit_pytorch.levit import LeViT\n",
    "\n",
    "LeViT = LeViT(\n",
    "    image_size = 128,\n",
    "    num_classes = 6,\n",
    "    stages = 3,             # number of stages\n",
    "    dim = (256, 384, 512),  # dimensions at each stage\n",
    "    depth = 4,              # transformer of depth 4 at each stage\n",
    "    heads = (4, 6, 8),      # heads at each stage\n",
    "    mlp_mult = 2,\n",
    "    dropout = 0.1\n",
    ").to(device)\n",
    "train_model(LeViT, \"LeViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGdibL--ps37"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Model 9: CvT\n",
    "from vit_pytorch.cvt import CvT\n",
    "\n",
    "CvT = CvT(\n",
    "    num_classes = 6,\n",
    "    s1_emb_dim = 64,        # stage 1 - dimension\n",
    "    s1_emb_kernel = 7,      # stage 1 - conv kernel\n",
    "    s1_emb_stride = 4,      # stage 1 - conv stride\n",
    "    s1_proj_kernel = 3,     # stage 1 - attention ds-conv kernel size\n",
    "    s1_kv_proj_stride = 2,  # stage 1 - attention key / value projection stride\n",
    "    s1_heads = 1,           # stage 1 - heads\n",
    "    s1_depth = 1,           # stage 1 - depth\n",
    "    s1_mlp_mult = 4,        # stage 1 - feedforward expansion factor\n",
    "    s2_emb_dim = 192,       # stage 2 - (same as above)\n",
    "    s2_emb_kernel = 3,\n",
    "    s2_emb_stride = 2,\n",
    "    s2_proj_kernel = 3,\n",
    "    s2_kv_proj_stride = 2,\n",
    "    s2_heads = 3,\n",
    "    s2_depth = 2,\n",
    "    s2_mlp_mult = 4,\n",
    "    s3_emb_dim = 384,       # stage 3 - (same as above)\n",
    "    s3_emb_kernel = 3,\n",
    "    s3_emb_stride = 2,\n",
    "    s3_proj_kernel = 3,\n",
    "    s3_kv_proj_stride = 2,\n",
    "    s3_heads = 4,\n",
    "    s3_depth = 10,\n",
    "    s3_mlp_mult = 4,\n",
    "    dropout = 0.\n",
    ").to(device)\n",
    "train_model(CvT, \"CvT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAXcgZBTpwnQ"
   },
   "outputs": [],
   "source": [
    "# Model 13: ScalableViT\n",
    "from vit_pytorch.scalable_vit import ScalableViT\n",
    "\n",
    "ScalableViT = ScalableViT(\n",
    "    num_classes = 6,\n",
    "    dim = 64,                               # starting model dimension. at every stage, dimension is doubled\n",
    "    heads = (2, 4, 8, 16),                  # number of attention heads at each stage\n",
    "    depth = (2, 2, 20, 2),                  # number of transformer blocks at each stage\n",
    "    ssa_dim_key = (40, 40, 40, 32),         # the dimension of the attention keys (and queries) for SSA. in the paper, they represented this as a scale factor on the base dimension per key (ssa_dim_key / dim_key)\n",
    "    reduction_factor = (8, 4, 2, 1),        # downsampling of the key / values in SSA. in the paper, this was represented as (reduction_factor ** -2)\n",
    "    window_size = (32, 16, None, None),     # window size of the IWSA at each stage. None means no windowing needed\n",
    "    dropout = 0.1,                          # attention and feedforward dropout\n",
    ").to(device)\n",
    "train_model(ScalableViT, \"ScalableViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFLpeoUZpz2z"
   },
   "outputs": [],
   "source": [
    "# Model 15: MobileViT\n",
    "from vit_pytorch.mobile_vit import MobileViT\n",
    "\n",
    "MobileViT = MobileViT(\n",
    "    image_size = (128, 128),\n",
    "    dims = [96, 120, 144],\n",
    "    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n",
    "    num_classes = 6\n",
    ").to(device)\n",
    "train_model(MobileViT, \"MobileViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ichf5ulp2W-"
   },
   "outputs": [],
   "source": [
    "# Model 16: SmallDataViT\n",
    "from vit_pytorch.vit_for_small_dataset import ViT as SmallDataViT\n",
    "\n",
    "SmallDataViT = SmallDataViT(\n",
    "    image_size = 128,\n",
    "    patch_size = 16,\n",
    "    num_classes = 6,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)\n",
    "train_model(SmallDataViT, \"SmallDataViT\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
