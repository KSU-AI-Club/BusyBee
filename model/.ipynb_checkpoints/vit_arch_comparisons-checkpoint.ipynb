{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c044dc-9fab-47cd-b855-261ac6947826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this within a python venv and have the error \"Module not found: numpy\", \n",
    "# ensure that your kernel is set to $VENV_NAME and run $python -m ipykernel install --user --name=$VENV_NAME \n",
    "import test_infra as infra\n",
    "import os\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b105fa-994f-4615-9d1a-1a8e58d2af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "\n",
    "# Training Settings\n",
    "epochs = 15 # Need to monitor validation loss during training to avoid overfitting https://datascience.stackexchange.com/questions/46523/is-a-large-number-of-epochs-good-or-bad-idea-in-cnn\n",
    "batch_size = 32 # This should usually be kept to a size that is a power of two\n",
    "lr = 3e-5 # Need to implement learning rate decay \n",
    "gamma = 0.7\n",
    "\n",
    "#hyperparams = infra.hyperparams(epochs, batch_size, lr, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b82511-8be8-4dbd-8a2a-1e6b4b97c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"VanillaViT\", \"SimpleViT\", \"T2TViT\", \"CrossViT\", \"PiT\", \"LeViT\", \"CvT\", \"MobileViT\", \"SmallDataViT\"]\n",
    "vit_experiment = infra.training_statistics(model_names, epochs, batch_size, lr, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f1eb1-1883-475b-8463-ffbb768b4610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "# Define the transformations that will be applied to the images during the loading process\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Mean and Std. Dev values used here are commonly used with the ImageNet dataset\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a006f2f-c6e9-418a-8b1e-d7b39e207bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to dataset\n",
    "dataset_path = \"%s/../data/ga_imgs/\" % (os.getcwd())\n",
    "vit_experiment.load_data(dataset_path, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a46cf-3067-4d6b-8323-b9b0bc8d5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VanillaViT\n",
    "# Need to get rid of magic numers here\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "VanillaViT = ViT(\n",
    "    image_size = 128,\n",
    "    patch_size = 8,\n",
    "    num_classes = 23,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)\n",
    "vit_experiment.train_model(VanillaViT, \"VanillaViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5a96e-7d62-45f4-94d9-77d29d542c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleViT \n",
    "from vit_pytorch import SimpleViT\n",
    "\n",
    "SimpleViT = SimpleViT(\n",
    "    image_size = 128,\n",
    "    patch_size = 8,\n",
    "    num_classes = 23,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048\n",
    ").to(device)\n",
    "train_model(SimpleViT, \"SimpleViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04363ef3-75da-48a4-8f50-b47af4b9b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2TViT\n",
    "from vit_pytorch.t2t import T2TViT\n",
    "\n",
    "T2TViT = T2TViT(\n",
    "    dim = 512,\n",
    "    image_size = 128,\n",
    "    depth = 5,\n",
    "    heads = 8,\n",
    "    mlp_dim = 512,\n",
    "    num_classes = 23,\n",
    "    t2t_layers = ((7, 4), (3, 2), (3, 2)) # tuples of the kernel size and stride of  each consecutive layers of the initial token to token module\n",
    ").to(device)\n",
    "train_model(T2TViT, \"T2TViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b3b6c-2a0a-4432-92db-7661d5d8befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossViT\n",
    "from vit_pytorch.cross_vit import CrossViT\n",
    "\n",
    "CrossViT = CrossViT(\n",
    "    image_size = 128,\n",
    "    num_classes = 23,\n",
    "    depth = 4,               # number of multi-scanvidia-smi --gpu-reset -i $nle encoding blocks\n",
    "    sm_dim = 192,            # high res dimension\n",
    "    sm_patch_size = 16,      # high res patch size (should be smaller than lg_patch_size)\n",
    "    sm_enc_depth = 2,        # high res depth\n",
    "    sm_enc_heads = 8,        # high res heads\n",
    "    sm_enc_mlp_dim = 2048,   # high res feedforward dimension\n",
    "    lg_dim = 384,            # low res dimension\n",
    "    lg_patch_size = 64,      # low res patch size\n",
    "    lg_enc_depth = 3,        # low res depth\n",
    "    lg_enc_heads = 8,        # low res heads\n",
    "    lg_enc_mlp_dim = 2048,   # low res feedforward dimensions\n",
    "    cross_attn_depth = 2,    # cross attention rounds\n",
    "    cross_attn_heads = 8,    # cross attention heads\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)\n",
    "train_model(CrossViT, \"CrossViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e4215-dbab-4279-9036-04ed687be8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PiT\n",
    "from vit_pytorch.pit import PiT\n",
    "\n",
    "PiT = PiT(\n",
    "    image_size = 128,\n",
    "    patch_size = 16,\n",
    "    dim = 256,\n",
    "    num_classes = 23,\n",
    "    depth = (3, 3, 3),     # list of depths, indicating the number of rounds of each stage before a downsample\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)\n",
    "train_model(PiT, \"PiT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0160f6-a680-4ade-9128-2c3a176ffb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeViT\n",
    "from vit_pytorch.levit import LeViT\n",
    "\n",
    "LeViT = LeViT(\n",
    "    image_size = 128,\n",
    "    num_classes = 23,\n",
    "    stages = 3,             # number of stages\n",
    "    dim = (256, 384, 512),  # dimensions at each stage\n",
    "    depth = 4,              # transformer of depth 4 at each stage\n",
    "    heads = (4, 6, 8),      # heads at each stage\n",
    "    mlp_mult = 2,\n",
    "    dropout = 0.1\n",
    ").to(device)\n",
    "train_model(LeViT, \"LeViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f227504-b390-40fb-9860-5d3a4758e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CvT\n",
    "from vit_pytorch.cvt import CvT\n",
    "\n",
    "CvT = CvT(\n",
    "    num_classes = 23,\n",
    "    s1_emb_dim = 64,        # stage 1 - dimension\n",
    "    s1_emb_kernel = 7,      # stage 1 - conv kernel\n",
    "    s1_emb_stride = 4,      # stage 1 - conv stride\n",
    "    s1_proj_kernel = 3,     # stage 1 - attention ds-conv kernel size\n",
    "    s1_kv_proj_stride = 2,  # stage 1 - attention key / value projection stride\n",
    "    s1_heads = 1,           # stage 1 - heads\n",
    "    s1_depth = 1,           # stage 1 - depth\n",
    "    s1_mlp_mult = 4,        # stage 1 - feedforward expansion factor\n",
    "    s2_emb_dim = 192,       # stage 2 - (same as above)\n",
    "    s2_emb_kernel = 3,\n",
    "    s2_emb_stride = 2,\n",
    "    s2_proj_kernel = 3,\n",
    "    s2_kv_proj_stride = 2,\n",
    "    s2_heads = 3,\n",
    "    s2_depth = 2,\n",
    "    s2_mlp_mult = 4,\n",
    "    s3_emb_dim = 384,       # stage 3 - (same as above)\n",
    "    s3_emb_kernel = 3,\n",
    "    s3_emb_stride = 2,\n",
    "    s3_proj_kernel = 3,\n",
    "    s3_kv_proj_stride = 2,\n",
    "    s3_heads = 4,\n",
    "    s3_depth = 10,\n",
    "    s3_mlp_mult = 4,\n",
    "    dropout = 0,\n",
    ").to(device)\n",
    "train_model(CvT, \"CvT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619548f0-7516-4c48-97c1-e3ae94f0d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ScalableViT\n",
    "from vit_pytorch.scalable_vit import ScalableViT\n",
    "\n",
    "ScalableViT = ScalableViT(\n",
    "    num_classes = 23,\n",
    "    dim = 64,                               # starting model dimension. at every stage, dimension is doubled\n",
    "    heads = (2, 4, 8, 16),                  # number of attention heads at each stage\n",
    "    depth = (2, 2, 20, 2),                  # number of transformer blocks at each stage\n",
    "    ssa_dim_key = (40, 40, 40, 32),         # the dimension of the attention keys (and queries) for SSA. in the paper, they represented this as a scale factor on the base dimension per key (ssa_dim_key / dim_key)\n",
    "    reduction_factor = (8, 4, 2, 1),        # downsampling of the key / values in SSA. in the paper, this was represented as (reduction_factor ** -2)\n",
    "    window_size = (32, 16, None, None),     # window size of the IWSA at each stage. None means no windowing needed\n",
    "    dropout = 0.1,                          # attention and feedforward dropout\n",
    ").to(device)\n",
    "train_model(ScalableViT, \"ScalableViT\")\n",
    "\n",
    "\n",
    "# Test Run \"runs out of memory\" here for some reason, despite not actually running out of memory \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab5c6e-567e-484b-8cf9-fc392c034ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileViT\n",
    "from vit_pytorch.mobile_vit import MobileViT\n",
    "\n",
    "MobileViT = MobileViT(\n",
    "    image_size = (128, 128),\n",
    "    dims = [96, 120, 144],\n",
    "    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n",
    "    num_classes = 23,\n",
    ").to(device)\n",
    "train_model(MobileViT, \"MobileViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1ed48-ca72-40cf-9079-851426677443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmallDataViT\n",
    "from vit_pytorch.vit_for_small_dataset import ViT as SmallDataViT\n",
    "\n",
    "SmallDataViT = SmallDataViT(\n",
    "    image_size = 128,\n",
    "    patch_size = 16,\n",
    "    num_classes = 23,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)\n",
    "train_model(SmallDataViT, \"SmallDataViT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49334d39-3a44-48fc-8765-781695c85ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855c970-8d9f-4be4-8109-4fe57c937b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef9e26-3b1d-4139-9d70-e1adfa94ce79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4f7ce-a216-4257-94ec-75a8a5ddd846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Convergence\n",
    "plt.figure(figsize=(20,10))\n",
    "for model in models:\n",
    "    acc_list = accuracy_dict[model]\n",
    "    plt.plot(acc_list, label=model)\n",
    "\n",
    "plt.xticks(range(0,epochs), labels=range(1,epochs+1))\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Accuracy (Percentage Points)\", fontsize=14)\n",
    "plt.title(\"Training Accuracy Convergence\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b437da65-a792-4219-8115-9150265a36d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c7447-f8dc-46a5-a807-7aa0623f8678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae05210-c81a-4e08-8149-9a0785b54d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Inference using model\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Classes defined in alphabetical order\n",
    "classes = [\n",
    "    \"Ammobatoidini\",\n",
    "    \"Andrenini\",\n",
    "    \"Anthidiini\",\n",
    "    \"Anthophorini\",\n",
    "    \"Apini\",\n",
    "    \"Augochlorini\",\n",
    "    \"Bombini\",\n",
    "    \"Calliopsini\",\n",
    "    \"Caupolicanini\",\n",
    "    \"Ceratinini\",\n",
    "    \"Emphorini\",\n",
    "    \"Epeolini\",\n",
    "    \"Eucerini\",\n",
    "    \"Halictini\",\n",
    "    \"Megachilini\",\n",
    "    \"Melectini\",\n",
    "    \"Nomadini\",\n",
    "    \"Osmiini\",\n",
    "    \"Panurgini\",\n",
    "    \"Protandrenini\",\n",
    "    \"Sphecodini\",\n",
    "    \"Xylocopini\",\n",
    "]\n",
    "\n",
    "# Load our demo images (taken from google images)\n",
    "demo_images = {\n",
    "    \"augochlorini.jpg\": \"Augochlorini\",\n",
    "    \"bombini.jpg\": \"Bombini\",\n",
    "    \"halictini.jpg\": \"Halictini\",\n",
    "    \"osmiini.jpg\": \"Osmiini\",\n",
    "    \"xylocopa.jpg\": \"Xylocopini\",\n",
    "}\n",
    "\n",
    "def demo(filename, ground_truth):\n",
    "\n",
    "    demo_image_path = \"{}/../data/demo_img/{}\".format(os.getcwd(), filename)\n",
    "    demo_img = Image.open(demo_image_path).resize((128,128), resample=0)\n",
    "\n",
    "    # Display image\n",
    "    display(demo_img)\n",
    "\n",
    "    # Outputs a vector displaying the models predictions\n",
    "    demo_img = TF.to_tensor(demo_img)\n",
    "    demo_img.unsqueeze_(0) #need to provide the batch dimension at dim0\n",
    "    demo_img = demo_img.to(device)\n",
    "    predictions = VanillaViT(demo_img)\n",
    "\n",
    "    # Get index of max value (most confident prediction)\n",
    "    tribe = torch.argmax(predictions)\n",
    "\n",
    "    print(\"The VanillaViT model believes this bee is a {}\".format(classes[tribe]))\n",
    "    print(\"Ground truth: This bee is a {}\".format(ground_truth))\n",
    "\n",
    "for key, value in demo_images.items(): \n",
    "    demo(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2bbfb-aa59-41f1-a156-24a2f57f3931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
