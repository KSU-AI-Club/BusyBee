{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AomZhAueQJh5"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.cuda as cuda # This import is for if you have an Nvidia GPU and run on your pc\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm.notebook import tqdm\n",
        "# import timm # This import provides a bunch of prebuilt image models you can use for experimentation/comparisons\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Settings\n",
        "# How many images we will process concurrently, try to use powers of 2, you may want to lower if you're getting the \"out of memory\" error\n",
        "batch_size = 32\n",
        "# The number of times we iterate over the entire dataset\n",
        "epochs = 5\n",
        "# Learning Rate: We may later want to utilize learning rate decay\n",
        "lr = 3e-5\n",
        "gamma = 0.7\n",
        "# For reproducible results\n",
        "seed = 8743749123"
      ],
      "metadata": {
        "id": "x_ZAA6wokDoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's get our dataset\n",
        "\n",
        "# Define the transformations\n",
        "# Need to add more and play around with different transformations\n",
        "# RandomCrop is probably a good data augmentation technique we'll want to use\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Need to troubleshoot INaturalist dataset -- Loads the whole dataset over 100GB and overflows the colab disk\n",
        "# Taxonomy structure: Domain: Eukaryota, Kingdom: Animalia, Phylum: Arthropoda, Class: Insecta, Order: Hymenoptera, Suborder: Apocrita, Superfamily: Apoidea, Epifamily: Anthophila\n",
        "#dataset = datasets.INaturalist(root='./data', version='2021_train_mini', target_type=\"genus\", transform=transforms.ToTensor(), download=True)\n",
        "#dset_size = len(dataset)\n",
        "\n",
        "\n",
        "\n",
        "# Load Data [Locally Stored]\n",
        "cwd = os.getcwd()\n",
        "path = \"%s/../data/\" %(cwd)\n",
        "dataset = datasets.ImageFolder(bg_dataset_path, transform=transform) # Automatically assigns labels based on sub-directory name\n",
        "\n",
        "# Generate 2 splits: Train (80%), Test (20%)\n",
        "# (No hyperparameter validation this time around)\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "test_size = dataset_size - train_size\n",
        "\n",
        "train, test = torch.utils.data.random_split(dataset, [train_size, test_size], generator=g)\n",
        "\n",
        "# Create Data Loaders for splits\n",
        "train_dl = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "test_dl = DataLoader(test, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "gkORSZeuROdc",
        "outputId": "3cd9d2c3-bbea-4b9d-ef2c-6caa3d53c88d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Union' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e212db421396>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Need to troubleshoot INaturalist dataset -- Loads the whole dataset over 100GB and overflows the colab disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Taxonomy structure: Domain: Eukaryota, Kingdom: Animalia, Phylum: Arthropoda, Class: Insecta, Order: Hymenoptera, Suborder: Apocrita, Superfamily: Apoidea, Epifamily: Anthophila\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtarget_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"genus\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"anthophila\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINaturalist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2021_train_mini'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Union' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Our classes will be the tribe names\n",
        "classes = [\"\"]"
      ],
      "metadata": {
        "id": "ZtcKoV__gVF9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, name):\n",
        "    # Loss Function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    # Scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++\")\n",
        "    print(f\"Training Run [Model: {name}]\")\n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++\")\n",
        "\n",
        "    # Training Time\n",
        "    start_event = cuda.Event(enable_timing=True)\n",
        "    end_event = cuda.Event(enable_timing=True)\n",
        "    # Begin Clock\n",
        "    start_event.record()\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_accuracy = 0\n",
        "\n",
        "        for data, label in tqdm(bg_train_dl if background else no_bg_train_dl):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            loss = criterion(output, label)\n",
        "            #training_loss[f\"{name}\"].append(loss)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            acc = (output.argmax(dim=1) == label).float().mean()\n",
        "            #training_accuracy[f\"{name}\"].append(acc)\n",
        "            epoch_accuracy += acc / len(bg_train_dl if background else no_bg_train_dl)\n",
        "            epoch_loss += loss / len(bg_train_dl if background else no_bg_train_dl)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1} - loss: {epoch_loss:.4f} - acc: {epoch_accuracy:.4f}\")\n",
        "\n",
        "    # End Clock\n",
        "    end_event.record()\n",
        "    cuda.synchronize() # Wait for GPU operations to complete\n",
        "    time = start_event.elapsed_time(end_event) / 1000 # Convert to seconds\n",
        "    num_examples = batch_size * len(bg_train_dl if background else no_bg_train_dl)\n",
        "    time_per_example = time / (num_examples * epochs)\n",
        "    print(f\"It took {time} seconds to train {name} on {num_examples} examples over {epochs} epochs.\")\n",
        "    print(f\"That averages to {time_per_example} seconds per example\")\n",
        "\n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++\")\n",
        "    print(f\"Test Run [Model: {name}] \")\n",
        "    print(\"++++++++++++++++++++++++++++++++++++++++\")\n",
        "    accuracies = []\n",
        "    batch_acc = 0\n",
        "    for data, label in tqdm(bg_test_dl if background else no_bg_test_dl):\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(data)\n",
        "        acc = (output.argmax(dim=1) == label).float().mean().cpu().detach().numpy()\n",
        "        batch_acc += acc / len(bg_test_dl if background else no_bg_test_dl)\n",
        "        accuracies.append(batch_acc)\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracies[-1]} - Number of test cases: {len(bg_test_dl if background else no_bg_test_dl) * batch_size}\")\n"
      ],
      "metadata": {
        "id": "2VU3evGWUM4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1: VanillaViT\n",
        "from vit_pytorch import ViT\n",
        "\n",
        "VanillaViT = ViT(\n",
        "    image_size = 128,\n",
        "    patch_size = 8,\n",
        "    num_classes = 6,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ").to(device)\n",
        "train_model(VanillaViT, \"VanillaViT\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"-------------------------------------------------------------\")\n",
        "print(f\"Attention Visualization\")\n",
        "print(f\"[Model: name]\")\n",
        "print(\"-------------------------------------------------------------\")\n",
        "print(\"\")\n",
        "bg_imgs, no_bg_imgs = load_jpgs()\n",
        "from vit_pytorch.recorder import Recorder\n",
        "v = Recorder(VanillaViT)\n",
        "\"\"\"\n",
        "# Experimenting with attention weight visualization\n",
        "\n",
        "for index, img in enumerate(bg_imgs):\n",
        "    img.unsqueeze(0)\n",
        "    preds, attns = v(img)\n",
        "    attns\n",
        "    attns.shape()\n",
        "    plot_mats(img, attns, 'test', categories[index], 'test_plt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "QlBLbmN6xOhw",
        "outputId": "8272b113-cb8b-4103-a4c0-cc14f2f524b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_dset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a167d8661e59>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Label (numeric):'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2: SimpleViT\n",
        "from vit_pytorch import SimpleViT\n",
        "\n",
        "SimpleViT = SimpleViT(\n",
        "    image_size = 128,\n",
        "    patch_size = 8,\n",
        "    num_classes = 6,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048\n",
        ").to(device)\n",
        "train_model(SimpleViT, \"SimpleViT\")"
      ],
      "metadata": {
        "id": "QwggSCNrYoMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 4: T2TViT\n",
        "from vit_pytorch.t2t import T2TViT\n",
        "\n",
        "T2TViT = T2TViT(\n",
        "    dim = 512,\n",
        "    image_size = 128,\n",
        "    depth = 5,\n",
        "    heads = 8,\n",
        "    mlp_dim = 512,\n",
        "    num_classes = 6,\n",
        "    t2t_layers = ((7, 4), (3, 2), (3, 2)) # tuples of the kernel size and stride of  each consecutive layers of the initial token to token module\n",
        ").to(device)\n",
        "train_model(T2TViT, \"T2TViT\")"
      ],
      "metadata": {
        "id": "TN3hzLfzdAm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 6: CrossViT\n",
        "from vit_pytorch.cross_vit import CrossViT\n",
        "\n",
        "CrossViT = CrossViT(\n",
        "    image_size = 128,\n",
        "    num_classes = 6,\n",
        "    depth = 4,               # number of multi-scale encoding blocks\n",
        "    sm_dim = 192,            # high res dimension\n",
        "    sm_patch_size = 16,      # high res patch size (should be smaller than lg_patch_size)\n",
        "    sm_enc_depth = 2,        # high res depth\n",
        "    sm_enc_heads = 8,        # high res heads\n",
        "    sm_enc_mlp_dim = 2048,   # high res feedforward dimension\n",
        "    lg_dim = 384,            # low res dimension\n",
        "    lg_patch_size = 64,      # low res patch size\n",
        "    lg_enc_depth = 3,        # low res depth\n",
        "    lg_enc_heads = 8,        # low res heads\n",
        "    lg_enc_mlp_dim = 2048,   # low res feedforward dimensions\n",
        "    cross_attn_depth = 2,    # cross attention rounds\n",
        "    cross_attn_heads = 8,    # cross attention heads\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ").to(device)\n",
        "train_model(CrossViT, \"CrossViT\")"
      ],
      "metadata": {
        "id": "rNUW5-P-d45J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "783e22f3-a681-4f5b-ddf4-d6f90477585e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'epochs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b636d4ab5c1c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Actually Training the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Load the image and tell pytorch to keep track of the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 7: PiT\n",
        "from vit_pytorch.pit import PiT\n",
        "\n",
        "PiT = PiT(\n",
        "    image_size = 128,\n",
        "    patch_size = 16,\n",
        "    dim = 256,\n",
        "    num_classes = 6,\n",
        "    depth = (3, 3, 3),     # list of depths, indicating the number of rounds of each stage before a downsample\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ").to(device)\n",
        "train_model(PiT, \"PiT\")"
      ],
      "metadata": {
        "id": "HWvXKdoVsx7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 8: LeViT\n",
        "from vit_pytorch.levit import LeViT\n",
        "\n",
        "LeViT = LeViT(\n",
        "    image_size = 128,\n",
        "    num_classes = 6,\n",
        "    stages = 3,             # number of stages\n",
        "    dim = (256, 384, 512),  # dimensions at each stage\n",
        "    depth = 4,              # transformer of depth 4 at each stage\n",
        "    heads = (4, 6, 8),      # heads at each stage\n",
        "    mlp_mult = 2,\n",
        "    dropout = 0.1\n",
        ").to(device)\n",
        "train_model(LeViT, \"LeViT\")"
      ],
      "metadata": {
        "id": "uITq_SmcppfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Model 9: CvT\n",
        "from vit_pytorch.cvt import CvT\n",
        "\n",
        "CvT = CvT(\n",
        "    num_classes = 6,\n",
        "    s1_emb_dim = 64,        # stage 1 - dimension\n",
        "    s1_emb_kernel = 7,      # stage 1 - conv kernel\n",
        "    s1_emb_stride = 4,      # stage 1 - conv stride\n",
        "    s1_proj_kernel = 3,     # stage 1 - attention ds-conv kernel size\n",
        "    s1_kv_proj_stride = 2,  # stage 1 - attention key / value projection stride\n",
        "    s1_heads = 1,           # stage 1 - heads\n",
        "    s1_depth = 1,           # stage 1 - depth\n",
        "    s1_mlp_mult = 4,        # stage 1 - feedforward expansion factor\n",
        "    s2_emb_dim = 192,       # stage 2 - (same as above)\n",
        "    s2_emb_kernel = 3,\n",
        "    s2_emb_stride = 2,\n",
        "    s2_proj_kernel = 3,\n",
        "    s2_kv_proj_stride = 2,\n",
        "    s2_heads = 3,\n",
        "    s2_depth = 2,\n",
        "    s2_mlp_mult = 4,\n",
        "    s3_emb_dim = 384,       # stage 3 - (same as above)\n",
        "    s3_emb_kernel = 3,\n",
        "    s3_emb_stride = 2,\n",
        "    s3_proj_kernel = 3,\n",
        "    s3_kv_proj_stride = 2,\n",
        "    s3_heads = 4,\n",
        "    s3_depth = 10,\n",
        "    s3_mlp_mult = 4,\n",
        "    dropout = 0.\n",
        ").to(device)\n",
        "train_model(CvT, \"CvT\")"
      ],
      "metadata": {
        "id": "cGdibL--ps37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 13: ScalableViT\n",
        "from vit_pytorch.scalable_vit import ScalableViT\n",
        "\n",
        "ScalableViT = ScalableViT(\n",
        "    num_classes = 6,\n",
        "    dim = 64,                               # starting model dimension. at every stage, dimension is doubled\n",
        "    heads = (2, 4, 8, 16),                  # number of attention heads at each stage\n",
        "    depth = (2, 2, 20, 2),                  # number of transformer blocks at each stage\n",
        "    ssa_dim_key = (40, 40, 40, 32),         # the dimension of the attention keys (and queries) for SSA. in the paper, they represented this as a scale factor on the base dimension per key (ssa_dim_key / dim_key)\n",
        "    reduction_factor = (8, 4, 2, 1),        # downsampling of the key / values in SSA. in the paper, this was represented as (reduction_factor ** -2)\n",
        "    window_size = (32, 16, None, None),     # window size of the IWSA at each stage. None means no windowing needed\n",
        "    dropout = 0.1,                          # attention and feedforward dropout\n",
        ").to(device)\n",
        "train_model(ScalableViT, \"ScalableViT\")"
      ],
      "metadata": {
        "id": "vAXcgZBTpwnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 15: MobileViT\n",
        "from vit_pytorch.mobile_vit import MobileViT\n",
        "\n",
        "MobileViT = MobileViT(\n",
        "    image_size = (128, 128),\n",
        "    dims = [96, 120, 144],\n",
        "    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n",
        "    num_classes = 6\n",
        ").to(device)\n",
        "train_model(MobileViT, \"MobileViT\")"
      ],
      "metadata": {
        "id": "wFLpeoUZpz2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 16: SmallDataViT\n",
        "from vit_pytorch.vit_for_small_dataset import ViT as SmallDataViT\n",
        "\n",
        "SmallDataViT = SmallDataViT(\n",
        "    image_size = 128,\n",
        "    patch_size = 16,\n",
        "    num_classes = 6,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ").to(device)\n",
        "train_model(SmallDataViT, \"SmallDataViT\")"
      ],
      "metadata": {
        "id": "0Ichf5ulp2W-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}